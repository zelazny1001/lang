import itertools
from collections import deque
import glob
import librosa
import math
import numpy as np
import os
import streamlit as st
import torch
from time import monotonic, monotonic_ns, time, sleep
import requests
from uuid import uuid4
import streamlit.components.v1 as components
import base64
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline
from vadv2 import SpanDetector, Span

def configure_environment(proxy_address, audio_dir):
    os.environ["no_proxy"] = proxy_address
    return audio_dir

def fetch_audio_files(audio_directory):
    return {f"Example {i}: ({file.replace(audio_directory, '').split('/')[-1]})": file 
            for i, file in enumerate(sorted(glob.glob(audio_directory + r'**/*.wav')))}

def setup_ui(audio_files):
    st.set_page_config(layout="wide")
    st.markdown("<h1 style='text-align:center; color:navy;'> Main Heading</h1>", unsafe_allow_html=True)
    st.sidebar.markdown("# The Smarts")
    st.sidebar.divider()
    
    audio_selection_container = st.columns(1)
    control_buttons_layout = st.columns([0.3, 0.2, 0.5])
    
    with audio_selection_container[0]:  # Renamed for better clarity
        selected_audio = st.selectbox('Pick a file', audio_files.keys(), key='audio_example')
    
    with control_buttons_layout[0]:
        analyze_audio = st.button(label='Analyze')
    with control_buttons_layout[1]:
        stop_processing = st.button(label='Stop')
    with control_buttons_layout[2]:
        with st.expander('Advanced Options'):
            audio_channel = st.radio('Customer audio channel', ['Channel 1', 'Channel 2'])
            summarize_each_segment = st.toggle('Enable Every Summary', value=False)
            summarize_entire_call = st.toggle('Enable Full Summary', value=True)
            enable_debug_logs = st.toggle('Enable Debug', value=False)
    
    st.divider()
    st.markdown('<h2 style="text-align: center;">Call Transcript</h2>', unsafe_allow_html=True)
    
    return selected_audio, analyze_audio, stop_processing, summarize_entire_call, enable_debug_logs

@st.cache_resource
def load_transcription_pipeline():
    model_path = '/data/location/dev2/poc/large'
    device = "cuda" if torch.cuda.is_available() else "cpu"
    precision = torch.float16 if torch.cuda.is_available() else torch.float32
    processor = AutoProcessor.from_pretrained(model_path)
    model = AutoModelForSpeechSeq2Seq.from_pretrained(model_path, torch_dtype=precision, use_safetensors=False).to(device)
    return pipeline("automatic-speech-recognition", model=model, tokenizer=processor.tokenizer,
                    feature_extractor=processor.feature_extractor, batch_size=1, torch_dtype=precision, device=device)

def fetch_summary(session_id):
    headers = {
        "Content-Type": "application/json",
        "Accept": "application/json"
    }
    return requests.get("https://the.full.summary.host/getFullSummary", 
                        params={"sessionID": session_id}, headers=headers, verify=False).text

def submit_transcription(session_id, speaker_id, transcript, start_time, end_time, is_final=False):
    headers = {
        "Content-Type": "application/json",
        "Accept": "application/json"
    }
    payload = {
        "sessionID": session_id,
        "personID": speaker_id,
        "transcript": transcript,
        "isEnd": is_final,
        "startTimestamp": start_time,
        "endTimestamp": end_time
    }
    return payload, requests.post("https://the.summary.host/grind.the.bits", headers=headers, json=payload, verify=False).json

def process_audio(selected_audio, stereo_audio, summarize_entire_call, enable_debug_logs, transcriber):
    chunk_duration = 1
    chunk_size = int(chunk_duration * 16000)
    total_chunks = math.ceil(len(stereo_audio[0]) / chunk_size)

    user_vad = SpanDetector(chunks_per_sec=16, energy_threshold_0=0.01, energy_threshold_1=0.02, 
                            min_voice_chunks=4, min_non_voice_chunks=4)
    ai_vad = SpanDetector(chunks_per_sec=16, energy_threshold_0=0.01, energy_threshold_1=0.02, 
                          min_voice_chunks=4, min_non_voice_chunks=4)
    speaker_timeout = 10
    silence_timeout = 4
    conversation_buffer = deque()

    session_id, start_timestamp = str(uuid4()), time()
    last_speaker = None
    chat_message = None

    for chunk_index in range(total_chunks):
        start = monotonic_ns()
        
        user_spans, _ = user_vad.add_to_stream(stereo_audio[0][chunk_index * chunk_size:(chunk_index + 1) * chunk_size], 'user')
        ai_spans, _ = ai_vad.add_to_stream(stereo_audio[1][chunk_index * chunk_size:(chunk_index + 1) * chunk_size], 'ai')
        detected_spans = sorted(user_spans + ai_spans, key=lambda x: x.start_idx)
        
        for span in detected_spans:
            transcribed_text = transcriber(stereo_audio[0 if span.speaker == 'user' else 1][span.start_idx:span.end_idx])['text']
            if transcribed_text.lower() in ["you", "ah", "bye."]:
                continue
            
            if last_speaker != span.speaker:
                last_speaker = span.speaker
                chat_message = st.chat_message(span.speaker)
            
            with chat_message:
                st.write(transcribed_text)
            
            submit_transcription(session_id, 'Person2' if span.speaker == 'user' else 'Person1', transcribed_text, 
                                 int(start_timestamp + span.start_idx / 16000), int(start_timestamp + span.end_idx / 16000))
        
        total_execution_time = (monotonic_ns() - start) * 1.0e-9
        sleep_time = chunk_index * chunk_duration - total_execution_time
        if sleep_time > 0:
            sleep(sleep_time)
        
        if enable_debug_logs:
            print(f"Chunk {chunk_index}: Execution Time: {total_execution_time:.2f}s, Sleep Time: {sleep_time:.2f}s")

    if summarize_entire_call:
        st.sidebar.markdown('**Intelligent Summary**')
        st.sidebar.write(fetch_summary(session_id))

def main():
    audio_directory = configure_environment("location.of.proxy.server", "/path/to/demo/last")
    audio_files = fetch_audio_files(audio_directory)
    selected_audio, analyze_audio, stop_processing, summarize_entire_call, enable_debug_logs = setup_ui(audio_files)
    transcriber = load_transcription_pipeline()

    if stop_processing:
        st.stop()

    if selected_audio:
        mono_audio, _ = librosa.load(audio_files[selected_audio], sr=16000, mono=True)
        stereo_audio, _ = librosa.load(audio_files[selected_audio], sr=16000, mono=False)
        st.audio(mono_audio, sample_rate=16000)

    if analyze_audio:
        process_audio(selected_audio, stereo_audio, summarize_entire_call, enable_debug_logs, transcriber)

if __name__ == "__main__":
    main()