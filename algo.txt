def load_exclusion_tokens(filename: str) -> set[str]:
    with open(filename, "r", encoding="utf-8") as f:
        return {line.strip() for line in f if line.strip()}

def write_distinct_tokens(tokens: set[str], output_filename: str) -> None:
    with open(output_filename, "w", encoding="utf-8") as f:
        for token in sorted(tokens):
            f.write(f"{token}\n")

def filter_input_file(
    input_filename: str,
    output_filename: str,
    exclusion_tokens: set[str],
) -> None:
    with open(input_filename, "r", encoding="utf-8") as infile, \
         open(output_filename, "w", encoding="utf-8") as outfile:
        for line in infile:
            if not any(token in line for token in exclusion_tokens):
                outfile.write(line)

def main() -> None:
    input_filename = "input.txt"
    exclusion_tokens_filename = "exclusion_tokens.txt"
    distinct_tokens_filename = "distinct_exclusion_tokens.txt"
    filtered_output_filename = "new_training_file.txt"

    exclusion_tokens = load_exclusion_tokens(exclusion_tokens_filename)
    write_distinct_tokens(exclusion_tokens, distinct_tokens_filename)
    filter_input_file(input_filename, filtered_output_filename, exclusion_tokens)

if __name__ == "__main__":
    main()